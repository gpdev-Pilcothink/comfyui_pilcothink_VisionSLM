# nodes/ollama_generator.py

from __future__ import annotations

import base64
import io
import json
from typing import Tuple

import torch
from PIL import Image  # type: ignore

try:
    import requests  # type: ignore
except ImportError:
    requests = None  # type: ignore

from .ollama_connective import OllamaConnection, _build_ollama_endpoint


def _image_to_base64(image_tensor: torch.Tensor) -> str:
    """
    ComfyUI IMAGE (torch.Tensor [B,H,W,C], 0~1 float)ì„
    Ollama /api/generate ì˜ images íŒŒë¼ë¯¸í„°ìš© base64 ë¬¸ìì—´ë¡œ ë³€í™˜.
    """
    if image_tensor is None:
        raise ValueError("image_tensorê°€ None ì…ë‹ˆë‹¤.")

    if not isinstance(image_tensor, torch.Tensor):
        raise TypeError(f"IMAGE íƒ€ì…ì´ torch.Tensorê°€ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤: {type(image_tensor)}")

    if image_tensor.dim() == 4:
        first = image_tensor[0]
    else:
        first = image_tensor

    first = first.clamp(0, 1)
    first = (first * 255.0).to(torch.uint8)

    arr = first.cpu().numpy()
    img = Image.fromarray(arr)
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    img_bytes = buf.getvalue()
    b64 = base64.b64encode(img_bytes).decode("utf-8")
    return b64


def _split_thinking_and_answer_from_text(text: str) -> Tuple[str, str]:
    """
    ì‘ë‹µ í…ìŠ¤íŠ¸ ì•ˆì— <think> ... </think> íŒ¨í„´ì´ ë“¤ì–´ ìˆì„ ë•Œ
    thinking/answer ë¥¼ ë¶„ë¦¬í•˜ëŠ” ë³´ì¡° ìœ í‹¸.
    """
    text = (text or "").strip()
    if not text:
        return "", ""

    close_tag = "</think>"
    open_tag = "<think>"

    idx_close = text.find(close_tag)
    if idx_close != -1:
        before = text[:idx_close]
        after = text[idx_close + len(close_tag):]

        idx_open = before.find(open_tag)
        if idx_open != -1:
            reasoning = before[idx_open + len(open_tag):]
        else:
            reasoning = before

        return reasoning.strip(), after.strip()

    idx_open = text.find(open_tag)
    if idx_open != -1:
        reasoning = text[idx_open + len(open_tag):]
        answer = text[:idx_open]
        return reasoning.strip(), answer.strip()

    return "", text


class OllamaGenerator:
    CATEGORY = "Pilcothink/Ollama"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "seed": ("INT", {"default": 0, "min": 0, "max": 999999}),
                "connection": ("OLLAMA_CONNECTION", {"forceInput": True}),
                "system_prompt": ("STRING", {"default": "", "multiline": True}),
                "prompt": ("STRING", {"default": "Describe the image.", "multiline": True}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.01}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_k": ("INT", {"default": 0, "min": 0, "max": 10000, "step": 1}),
                "repeat_penalty": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.01}),
                "keep_alive": ("STRING", {"default": "", "multiline": False}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 8192, "step": 1}),
            },
            "optional": {
                "image": ("IMAGE",),
            },
        }

    RETURN_TYPES = ("STRING", "STRING",)
    RETURN_NAMES = ("text", "thinking",)
    FUNCTION = "generate"
    OUTPUT_NODE = True

    def generate(
        self,
        seed,
        connection: OllamaConnection,
        system_prompt: str,
        prompt: str,
        temperature: float,
        top_p: float,
        top_k: int,
        repeat_penalty: float,
        keep_alive: str,
        max_tokens: int,
        image=None,
    ):
        if requests is None:
            raise RuntimeError(
                "Python 'requests' íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. "
                "ComfyUIê°€ ì„¤ì¹˜ëœ íŒŒì´ì¬ í™˜ê²½ì— 'pip install requests'ë¡œ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”."
            )

        if not isinstance(connection, OllamaConnection):
            raise RuntimeError(
                "OLLAMA_CONNECTION íƒ€ì…ì´ ì•„ë‹Œ ê°’ì´ ë“¤ì–´ì™”ìŠµë‹ˆë‹¤. "
                "ë¨¼ì € Ollama Connective ë…¸ë“œë¥¼ ì‚¬ìš©í•´ì„œ ì—°ê²°ì„ ë§Œë“  ë’¤ ê·¸ ì¶œë ¥ì„ ì—°ê²°í•´ ì£¼ì„¸ìš”."
            )

        base_url = connection.base_url
        api_key = (connection.api_key or "").strip()
        model = (connection.model or "").strip()

        if not model:
            if connection.models:
                model = connection.models[0]
            else:
                raise RuntimeError("OllamaConnection ì•ˆì— ì„ íƒëœ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        headers = {
            "Content-Type": "application/json",
        }
        if api_key and api_key.upper() != "EMPTY":
            headers["Authorization"] = f"Bearer {api_key}"

        images = []
        if image is not None:
            images.append(_image_to_base64(image))

        system_prompt = (system_prompt or "").strip()
        user_prompt = (prompt or "")

        if not user_prompt and not images:
            raise RuntimeError("í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ëª¨ë‘ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        
        options = {
            "temperature": float(temperature),
            "top_p": float(top_p),
            "num_predict": int(max_tokens),
        }

        # âœ… top_k: 0ì´ë©´ key ìì²´ë¥¼ ë„£ì§€ ì•ŠìŒ
        tk = int(top_k)
        if tk > 0:
            options["top_k"] = tk

        # âœ… repeat_penalty: 0ì´ë©´ key ìì²´ë¥¼ ë„£ì§€ ì•ŠìŒ
        rp = float(repeat_penalty)
        if rp > 0:
            options["repeat_penalty"] = rp

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        user_msg = {"role": "user", "content": user_prompt}
        if images:
            user_msg["images"] = images
        messages.append(user_msg)

        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "options": options,
        }


        ka = (keep_alive or "").strip()
        if ka != "":
            if ka.lstrip("-").isdigit():
                payload["keep_alive"] = int(ka)
            else:
                payload["keep_alive"] = ka

        url = _build_ollama_endpoint(base_url, "/chat")

        thinking_parts = []
        answer_parts = []

        try:
            with requests.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=120,
            ) as resp:
                if resp.status_code != 200:
                    raise RuntimeError(
                        f"Ollama /generate ì‘ë‹µ ì˜¤ë¥˜ {resp.status_code}: {resp.text[:500]}"
                    )

                for line in resp.iter_lines(decode_unicode=True):
                    if not line:
                        continue
                    try:
                        item = json.loads(line)
                    except Exception:
                        continue

                    if "error" in item:
                        raise RuntimeError(str(item["error"]))

                     # /api/chat ìŠ¤íŠ¸ë¦¼ì€ message ì•ˆì— content/thinking ì´ ë“¤ì–´ì˜´
                    msg = item.get("message") or {}

                    # ğŸ”¹ 1) reasoning í† í°
                    t = msg.get("thinking", "") or ""
                    if t:
                        thinking_parts.append(t)

                    # ğŸ”¹ 2) ì‹¤ì œ ë‹µë³€ í† í°
                    c = msg.get("content", "") or ""
                    if c:
                        answer_parts.append(c)

                    if item.get("done"):
                        break

        except Exception as e:
            raise RuntimeError(f"Ollama /generate ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")

        thinking_text = "".join(thinking_parts).strip()
        answer_text = "".join(answer_parts).strip()

        # ë§Œì•½ thinking í•„ë“œê°€ ì—†ëŠ” ëª¨ë¸ì¸ë° <think>...</think> í˜•ì‹ì„ ì“´ë‹¤ë©´ fallback
        if not thinking_text and "<think>" in answer_text:
            th2, ans2 = _split_thinking_and_answer_from_text(answer_text)
            if th2:
                thinking_text = th2
                answer_text = ans2

        print(f"[OllamaGenerator] model={model}, max_tokens={max_tokens}")
        return (answer_text, thinking_text,)